{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0291b29a-65c5-4598-bca5-9c8f2cf31688",
   "metadata": {},
   "source": [
    "# 第一章: 自然语言处理基础篇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f070da-4284-416c-aa01-9bb2f6b4db2b",
   "metadata": {},
   "source": [
    "## 自然语言处理概述\n",
    "\n",
    "### 什么是自然语言处理\n",
    "\n",
    "#### 定义\n",
    "自然语言处理指的是使用计算机处理人类的语言,Natural Language Processing . 简称NLP\n",
    "\n",
    "#### 自然语言处理的分类\n",
    "- 序列标注: 给句子或篇章中的每个词或字一个标签. 如分词,词性标注.\n",
    "- 文本分类: 给每个句子或篇章一个标签,如情感分析.\n",
    "- 关系判断: 判断多个词语,句子,篇章之间的关系,如选词填空.\n",
    "- 语言生成: 产生自然语言的字,词,句子,篇章. 如问答系统,机器翻译.\n",
    "\n",
    "#### NLP的机器学习\n",
    "- Word2vec 可以从语料中自主学习得出每个词语的向量表示.\n",
    "- Seq2Seq \n",
    "- BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "#### NLP中的常用技术\n",
    "1. TF-IDF  \n",
    "词频-逆文本频率 (Term Frequency-Inverse Document Frequency TF-IDF),用于评估一个词在一定范围的语料中的重要程度.  \n",
    "词频是指一个词在一定范围的语料中出现的次数. 这个词在某语料中出现的次数越多说明它越重要,但是这个词有可能是\"的\" \"了\" 这样的在所有语料中出现次数都很多的词.所以就出现了逆文本频率, 就是这个词在某个语料中出现了,但是在某个语料库中出现得很少,就能说明这个词在这个语料中重要.\n",
    "\n",
    "2. 词嵌入  \n",
    "词嵌入(Word Embedding) 就是用向量表示词语. 在文字处理软件中,字符往往用一个数字编码表示, 如ASCII中大写字母'A'用65表示.做自然语言处理认为时,我们需要用计算机能理解的符号表示字或词,但问题是词语的数量很多,而且词语之间是有语义关系的,单纯的用数字编号难以表达这种复杂的语义关系.  \n",
    "词嵌入就是使用多维向量表示一个词语,这样词语之间的关系可以用向量间的关系来反映.词嵌入需要特定的算法,可在语料库训练得到.\n",
    "\n",
    "3. 分词  \n",
    "分词是指把句子划分为词语序列.\n",
    "\n",
    "4. 循环神经网络  \n",
    "循环神经网络(Recurrent Neural Network, RNN) 模型是用于处理序列数据的神经网络,它可以处理不定长度的数据. 因为NLP过程中我们常常把句子经过分词变成了一个序列,而实际中的句子长短各异, 所以适合用RNN模型处理.   \n",
    "RNN模型也可以用于生成不定长或定长数据.\n",
    "\n",
    "5. Seq2seq  \n",
    "Seq2seq (Sequence to sequence), 即序列到序列, 是一种输入和输出都是不定长序列的模型,可以用于机器翻译,问答系统.\n",
    "\n",
    "6. 注意力机制  \n",
    "注意力机制(Attention Mechanism) 源于人们对人类视觉机制的研究,人类观察事物时,会把注意力分配到关键的地方,而相对忽视其他细节. 在NLP 中可以认为,如果使用注意力机制,模型会给重要的词语分配更高的权重, 或者把句子中某些关系密切的词语关联起来共同考虑.  \n",
    "\n",
    "7. 预训练  \n",
    "预训练是一种迁移学习方法. 如BERT 模型就是预训练模型.\n",
    "\n",
    "8. 多模态学习  \n",
    "多模态(Multimodal)学习指模型可以用于同时处理相关的不同形式的信息.常见的有视觉信息和文字信息, 如同时处理图片和图片的描述的模型.\n",
    "\n",
    "#### 机器学习中的常见问题\n",
    "\n",
    "##### 1. Batch 和 Epoch\n",
    "Batch指每次更新模型参数时所使用或依据的一批数据.训练模型使用的方法被称为梯度下降(Gradient Desent), 即把一批数据输入模型求出损失,计算参数的导数,然后根据学习率朝梯度下降的方向整体更新参数,这一批数据就是Batch.  \n",
    "训练模型时常常要考虑Batch Size,即每次使用多少数据更新模型参数. 传统机器学习使用Batch Gradient Desent(BGD) 方法,每次使用全部数据集上的数据计算梯度.  深度学习中常用的是随机梯度下降(Stochastic Gradient Desent, SGD)方法,每次随机选取一部分数据训练模型.  \n",
    "Epoch 则是指一个训练的轮次, 一般每个轮次都会遍历整个数据集. 每个轮次可能会使用多个Batch进行训练.\n",
    "\n",
    "##### 2. Batch Size的选择\n",
    "Batch Size 不能太小,否则会导致有的模型无法收敛,而且选择大的Batch Size可以提高模型训练时的并行性能,前提是系统拥有足够的并行资源.  \n",
    "Batch Size不是越大越好. 在很多问题上,能得到最佳效果的Batch Size在2到32之间, 但最佳的Batch Size 并不总是固定的,而且大的Batch Size 需要系统资源充足. 如果显存资源不够,但是需要使用较大的Batch Size, 可以使用梯度累积, 即每执行N次模型后更新一次模型参数, 这就相当于实际的Batch Size 是设定的N倍, 但无法提高并行性能.\n",
    "\n",
    "##### 3. 数据集不平衡问题\n",
    "很多时候我们我们可能会遇到数据集中的数据分布不均匀的问题.数据不平衡的情况下模型可能会更倾向于数据中次数多的类别.\n",
    "\n",
    "##### 4. 预训练模型与数据安全\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5291a35-a475-4f03-8465-183b7318540d",
   "metadata": {},
   "source": [
    "# 第二章: Python 自然语言处理基础\n",
    "## 常用库\n",
    "1. NumPy \n",
    "2. Matplotlib\n",
    "3. scikit-learn\n",
    "4. NLTK\n",
    "5. spaCy\n",
    "6. jieba \n",
    "7. pkuseg\n",
    "8. wn\n",
    "\n",
    "## 处理语料\n",
    "- 去重 ,Set, 大数据去重可以考虑使用BitMap,或者布隆过滤器(Bloom Filter)\n",
    "- 停用词,stop words 是指规定的一个语料中频繁使用的词语或不含明确信息的词语, 如中文的\"一些\", 英文中的\"the\", \"a\",\"an\".\n",
    "- 编辑距离, 衡量两个字符串间差异的一种度量.定义了3种基本操作:  插入一个字符,删除一个字符,替换一个字符. 两个字符串间的编辑距离就是把一个字符串变成另一个字符串所需的最少基本操作的部署.\n",
    "\n",
    "### 编辑距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40131c43-5e46-4f11-8c07-07e6bb9871c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minDistance(word1: str, word2: str) -> int:\n",
    "    n = len(word1) # 字符串1的长度\n",
    "    m = len(word2) # 字符串2的长度\n",
    "    dp = [ [0] * (m+1) for _ in range(n+1) ]\n",
    "    for i in range(m+1): dp[0][i] = i \n",
    "    for i in range(m+1): dp[i][0] = i\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            if word1[i-1]  == word2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i][j-1], dp[i-1][j], dp[i-1][j-1])+1\n",
    "    return dp[-1][-1]\n",
    "\n",
    "minDistance(\"Hello\",\"World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfcd766-0a13-4a49-b4c1-64ab9e2b6b67",
   "metadata": {},
   "source": [
    "### 文本规范化\n",
    "文本规范化即 Text Normalization, 即按照某种方法对语料进行转换,清洗和标准化.例如去掉语料中多余的空白和停用词, 统一英文语料单词单复数,过去式. 下面是BERT-KPE中的英文文本规范化代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614f603b-b6e6-484b-9a12-9d7af42b1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "class DEL_ASCII(object):\n",
    "    ''' 在 `refactor_text_vdom` 中被使用,用于过滤掉字符: b'\\xef\\xb8\\x8f' '''\n",
    "    def do(self, text):\n",
    "        orig_tokens = self.whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_prunc(token))\n",
    "        output_tokens = self.whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "    \n",
    "    def whitespace_tokenize(self, text):\n",
    "        ''' 清理空白 并按单词切分句子 '''\n",
    "        text = text.strip() # 去除首尾空格,换行符,分隔符等空格\n",
    "        if not text: \n",
    "            return []\n",
    "        return text.split()\n",
    "    \n",
    "    def _run_strip_accents(self, text):\n",
    "        ''' 去掉重音符号 '''\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char) # 获取字符的类别\n",
    "            if cat == 'Mn': # Mark Nonspacing\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "    \n",
    "    def _run_split_on_prunc(self, text):\n",
    "        ''' 切分标点符号 '''\n",
    "        chars = list(text)\n",
    "        i = 0 \n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if self._is_punctuation(char): # 如果非数字,字母,空格\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "        return [\"\".join(x) for x in output]\n",
    "    \n",
    "    def _is_punctuation(self, char):\n",
    "        ''' 检查一个字符是否是标点符号 '''\n",
    "        cp = ord(char)\n",
    "        # 把所有非字母,非数字,非空格的ASCII 字符看成标点\n",
    "        if (cp > 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126) :\n",
    "            return True\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat.startswith('P'):\n",
    "            return True\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7624cc8d-524f-45bc-8983-9559e2249939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', ',', 'I', 'submitted', 'my', 'resume', '.']\n"
     ]
    }
   ],
   "source": [
    "del_ascii = DEL_ASCII()\n",
    "print(del_ascii.do('   Today , I    submitted my rèsumé.   '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3c562-2c8b-4814-a2e6-2a27873be6ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 分词\n",
    "英文分词没有什么难度, 但是中文比较困难.常用中文分词方法:    \n",
    "1. 基于字符串匹配的分词方法\n",
    "又称为机械分词方法, 首先需要定义一个词表,表中包含当前语料中的全部词语. 然后按照一定的规则扫描待分词的文本, 匹配到表中的词语就把它切分开来.扫描规则可分为3种: 正向最大匹配, 即从开头向结尾扫描; 逆向最大扫描, 即从结尾向开头扫描;最少切分,即尝试每句话切分出最少的词语.  \n",
    "2. 基于统计的分词方法.\n",
    "在一大段语料中统计字与字或者词与词的上下文关系,统计字或词共同出现的次数.然后对于要切分的文本,可以按照这个已经统计到的出现次数,选择概率尽可能大的切分方法.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a500502-4718-4fb9-ad70-284ef5cb3e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSpliter(object):\n",
    "    def __init__(self, corpus_path, encoding='utf-8', max_load_word_length=4):\n",
    "        self.dict = {}\n",
    "        self.dict2 = {}\n",
    "        self.max_word_length = 1\n",
    "        begin_time = time.time()\n",
    "        \n",
    "        with open(corpus_path, 'r',encoding=encoding) as f:\n",
    "            for l in f:\n",
    "                l.replace('[','')\n",
    "                l.replace(']','')\n",
    "                wds = l.strip().split(' ')\n",
    "                last_wd = ''\n",
    "                for i in range(1, len(wds)): \n",
    "                    try:\n",
    "                        wd, wtype = wds[i].split('/')\n",
    "                    except:\n",
    "                        continue\n",
    "                    if len(wd) == 0 or len(wd) > max_load_word_length or not wd.isalpha():\n",
    "                        continue\n",
    "                    if wd not in self.dict:\n",
    "                        self.dict[wd] = 0\n",
    "                        if len(wd) > self.max_word_length:\n",
    "                            self.max_word_length = len(wd)\n",
    "                            print(f'max_load_word_length={self.max_word_length} ,word is {wd}')\n",
    "                    self.dict[wd] += 1\n",
    "                    if last_wd:\n",
    "                        if last_wd+':'+wd not in self.dict2:\n",
    "                            self.dict2[last_wd+':'+wd] = 0\n",
    "                        self.dict2[last_wd+':'+wd] += 1\n",
    "                    last_wd = wd\n",
    "                self.words_cnt = 0\n",
    "                max_c = 0\n",
    "                for wd in self.dict:\n",
    "                    self.words_cnt += self.dict[wd]\n",
    "                    if self.dict[wd] > max_c:\n",
    "                        max_c = self.dict[wd]\n",
    "                self.words2_cnt = sum(self.dict2.values())\n",
    "                print('load corpus finished!') \n",
    "                print(f'{len(self.dict)} words in dict and frequency is : {self.words_cnt} ')\n",
    "                print(f'{len(self.dict2)} words in dict2 and frequency is : {self.words2_cnt} ')\n",
    "                print(f'spend { time.time() - begin_time } seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f6a4b-7413-4bd4-af36-584921e7f7fd",
   "metadata": {},
   "source": [
    "### 词频-逆文本频率\n",
    "scikit-learn 中提供了计算TF-IDF的类 TfidfVectorizer.\n",
    "###  One-Hot 编码\n",
    "使用神经网络模型时,一般需要使用向量表示自然语言中的符号,也就是词或者字,最简单的表示方法是One-Hot编码. One-Hot编码是先遍历语料,找出所有的字或词,例如与10个词,对其进行编号,从1到10,每一个数字代表一个词语,转换成向量则每个词都是10维向量, 每个向量只有1位为1, 其余位为0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7041fb-822b-41dd-8b0f-70effd5d5ee2",
   "metadata": {},
   "source": [
    "# 第五章: RNN分类帖子\n",
    "\n",
    "不怎么好的书,没有给数据集,只能自己去造了, 从b站获取了一些评论以及弹幕."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "242d5eb0-37c4-4672-b96a-741d40eee00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments = pd.read_csv('bilibili-comment/comments.csv')\n",
    "danmus = pd.read_csv('bilibili-comment/danmaku.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8210426c-6e4f-450b-9006-c9b8a2c6300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = comments['content']\n",
    "danmu_msg = danmus['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6440b-07f2-4aa0-97e7-e45a4ed9c9d4",
   "metadata": {},
   "source": [
    "## 5.2 输入与输出\n",
    "使用字符级RNN模型, 需要考虑如何把原始数据转换为模型可以接受的数据格式. \n",
    "\n",
    "### 5.2.1 统计数据集中出现的字符数量\n",
    "不论是使用One-Hot 表示法还是词嵌入,都需要先知道数据集中一共出现了多少个不同的字符. 假设出现了$N$个不同字符, 然后添加一个对应未知字符的特殊字符$UNK$, 那么使用 One-Hot 表示法,其中的每个字符对应向量的长度为$N+1$ . 用于统计数据集中出现字符数量的代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a32df16a-5b50-4bf1-b9f8-4faa6ce5899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4098\n"
     ]
    }
   ],
   "source": [
    "char_set = set() \n",
    "for content in contents:\n",
    "    for ch in content:\n",
    "        char_set.add(ch)\n",
    "for content in danmu_msg:\n",
    "    for ch in content:\n",
    "        char_set.add(ch)\n",
    "print(len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db18348-391e-4d2f-8f1f-3262ba7094e2",
   "metadata": {},
   "source": [
    "### 5.2.2 使用One-Hot编码表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf8cb302-5f2e-40a3-90aa-8202601113fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b347446-4072-448e-bdc7-b5739ce87fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
