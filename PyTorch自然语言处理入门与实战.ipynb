{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0291b29a-65c5-4598-bca5-9c8f2cf31688",
   "metadata": {},
   "source": [
    "# 第一章: 自然语言处理基础篇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f070da-4284-416c-aa01-9bb2f6b4db2b",
   "metadata": {},
   "source": [
    "## 自然语言处理概述\n",
    "\n",
    "### 什么是自然语言处理\n",
    "\n",
    "#### 定义\n",
    "自然语言处理指的是使用计算机处理人类的语言,Natural Language Processing . 简称NLP\n",
    "\n",
    "#### 自然语言处理的分类\n",
    "- 序列标注: 给句子或篇章中的每个词或字一个标签. 如分词,词性标注.\n",
    "- 文本分类: 给每个句子或篇章一个标签,如情感分析.\n",
    "- 关系判断: 判断多个词语,句子,篇章之间的关系,如选词填空.\n",
    "- 语言生成: 产生自然语言的字,词,句子,篇章. 如问答系统,机器翻译.\n",
    "\n",
    "#### NLP的机器学习\n",
    "- Word2vec 可以从语料中自主学习得出每个词语的向量表示.\n",
    "- Seq2Seq \n",
    "- BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "#### NLP中的常用技术\n",
    "1. TF-IDF  \n",
    "词频-逆文本频率 (Term Frequency-Inverse Document Frequency TF-IDF),用于评估一个词在一定范围的语料中的重要程度.  \n",
    "词频是指一个词在一定范围的语料中出现的次数. 这个词在某语料中出现的次数越多说明它越重要,但是这个词有可能是\"的\" \"了\" 这样的在所有语料中出现次数都很多的词.所以就出现了逆文本频率, 就是这个词在某个语料中出现了,但是在某个语料库中出现得很少,就能说明这个词在这个语料中重要.\n",
    "\n",
    "2. 词嵌入  \n",
    "词嵌入(Word Embedding) 就是用向量表示词语. 在文字处理软件中,字符往往用一个数字编码表示, 如ASCII中大写字母'A'用65表示.做自然语言处理认为时,我们需要用计算机能理解的符号表示字或词,但问题是词语的数量很多,而且词语之间是有语义关系的,单纯的用数字编号难以表达这种复杂的语义关系.  \n",
    "词嵌入就是使用多维向量表示一个词语,这样词语之间的关系可以用向量间的关系来反映.词嵌入需要特定的算法,可在语料库训练得到.\n",
    "\n",
    "3. 分词  \n",
    "分词是指把句子划分为词语序列.\n",
    "\n",
    "4. 循环神经网络  \n",
    "循环神经网络(Recurrent Neural Network, RNN) 模型是用于处理序列数据的神经网络,它可以处理不定长度的数据. 因为NLP过程中我们常常把句子经过分词变成了一个序列,而实际中的句子长短各异, 所以适合用RNN模型处理.   \n",
    "RNN模型也可以用于生成不定长或定长数据.\n",
    "\n",
    "5. Seq2seq  \n",
    "Seq2seq (Sequence to sequence), 即序列到序列, 是一种输入和输出都是不定长序列的模型,可以用于机器翻译,问答系统.\n",
    "\n",
    "6. 注意力机制  \n",
    "注意力机制(Attention Mechanism) 源于人们对人类视觉机制的研究,人类观察事物时,会把注意力分配到关键的地方,而相对忽视其他细节. 在NLP 中可以认为,如果使用注意力机制,模型会给重要的词语分配更高的权重, 或者把句子中某些关系密切的词语关联起来共同考虑.  \n",
    "\n",
    "7. 预训练  \n",
    "预训练是一种迁移学习方法. 如BERT 模型就是预训练模型.\n",
    "\n",
    "8. 多模态学习  \n",
    "多模态(Multimodal)学习指模型可以用于同时处理相关的不同形式的信息.常见的有视觉信息和文字信息, 如同时处理图片和图片的描述的模型.\n",
    "\n",
    "#### 机器学习中的常见问题\n",
    "\n",
    "##### 1. Batch 和 Epoch\n",
    "Batch指每次更新模型参数时所使用或依据的一批数据.训练模型使用的方法被称为梯度下降(Gradient Desent), 即把一批数据输入模型求出损失,计算参数的导数,然后根据学习率朝梯度下降的方向整体更新参数,这一批数据就是Batch.  \n",
    "训练模型时常常要考虑Batch Size,即每次使用多少数据更新模型参数. 传统机器学习使用Batch Gradient Desent(BGD) 方法,每次使用全部数据集上的数据计算梯度.  深度学习中常用的是随机梯度下降(Stochastic Gradient Desent, SGD)方法,每次随机选取一部分数据训练模型.  \n",
    "Epoch 则是指一个训练的轮次, 一般每个轮次都会遍历整个数据集. 每个轮次可能会使用多个Batch进行训练.\n",
    "\n",
    "##### 2. Batch Size的选择\n",
    "Batch Size 不能太小,否则会导致有的模型无法收敛,而且选择大的Batch Size可以提高模型训练时的并行性能,前提是系统拥有足够的并行资源.  \n",
    "Batch Size不是越大越好. 在很多问题上,能得到最佳效果的Batch Size在2到32之间, 但最佳的Batch Size 并不总是固定的,而且大的Batch Size 需要系统资源充足. 如果显存资源不够,但是需要使用较大的Batch Size, 可以使用梯度累积, 即每执行N次模型后更新一次模型参数, 这就相当于实际的Batch Size 是设定的N倍, 但无法提高并行性能.\n",
    "\n",
    "##### 3. 数据集不平衡问题\n",
    "很多时候我们我们可能会遇到数据集中的数据分布不均匀的问题.数据不平衡的情况下模型可能会更倾向于数据中次数多的类别.\n",
    "\n",
    "##### 4. 预训练模型与数据安全\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5291a35-a475-4f03-8465-183b7318540d",
   "metadata": {},
   "source": [
    "# 第二章: Python 自然语言处理基础\n",
    "## 常用库\n",
    "1. NumPy \n",
    "2. Matplotlib\n",
    "3. scikit-learn\n",
    "4. NLTK\n",
    "5. spaCy\n",
    "6. jieba \n",
    "7. pkuseg\n",
    "8. wn\n",
    "\n",
    "## 处理语料\n",
    "- 去重 ,Set, 大数据去重可以考虑使用BitMap,或者布隆过滤器(Bloom Filter)\n",
    "- 停用词,stop words 是指规定的一个语料中频繁使用的词语或不含明确信息的词语, 如中文的\"一些\", 英文中的\"the\", \"a\",\"an\".\n",
    "- 编辑距离, 衡量两个字符串间差异的一种度量.定义了3种基本操作:  插入一个字符,删除一个字符,替换一个字符. 两个字符串间的编辑距离就是把一个字符串变成另一个字符串所需的最少基本操作的部署.\n",
    "\n",
    "### 编辑距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40131c43-5e46-4f11-8c07-07e6bb9871c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minDistance(word1: str, word2: str) -> int:\n",
    "    n = len(word1) # 字符串1的长度\n",
    "    m = len(word2) # 字符串2的长度\n",
    "    dp = [ [0] * (m+1) for _ in range(n+1) ]\n",
    "    for i in range(m+1): dp[0][i] = i \n",
    "    for i in range(m+1): dp[i][0] = i\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            if word1[i-1]  == word2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i][j-1], dp[i-1][j], dp[i-1][j-1])+1\n",
    "    return dp[-1][-1]\n",
    "\n",
    "minDistance(\"Hello\",\"World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfcd766-0a13-4a49-b4c1-64ab9e2b6b67",
   "metadata": {},
   "source": [
    "### 文本规范化\n",
    "文本规范化即 Text Normalization, 即按照某种方法对语料进行转换,清洗和标准化.例如去掉语料中多余的空白和停用词, 统一英文语料单词单复数,过去式. 下面是BERT-KPE中的英文文本规范化代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614f603b-b6e6-484b-9a12-9d7af42b1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "class DEL_ASCII(object):\n",
    "    ''' 在 `refactor_text_vdom` 中被使用,用于过滤掉字符: b'\\xef\\xb8\\x8f' '''\n",
    "    def do(self, text):\n",
    "        orig_tokens = self.whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_prunc(token))\n",
    "        output_tokens = self.whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "    \n",
    "    def whitespace_tokenize(self, text):\n",
    "        ''' 清理空白 并按单词切分句子 '''\n",
    "        text = text.strip() # 去除首尾空格,换行符,分隔符等空格\n",
    "        if not text: \n",
    "            return []\n",
    "        return text.split()\n",
    "    \n",
    "    def _run_strip_accents(self, text):\n",
    "        ''' 去掉重音符号 '''\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char) # 获取字符的类别\n",
    "            if cat == 'Mn': # Mark Nonspacing\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "    \n",
    "    def _run_split_on_prunc(self, text):\n",
    "        ''' 切分标点符号 '''\n",
    "        chars = list(text)\n",
    "        i = 0 \n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if self._is_punctuation(char): # 如果非数字,字母,空格\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "        return [\"\".join(x) for x in output]\n",
    "    \n",
    "    def _is_punctuation(self, char):\n",
    "        ''' 检查一个字符是否是标点符号 '''\n",
    "        cp = ord(char)\n",
    "        # 把所有非字母,非数字,非空格的ASCII 字符看成标点\n",
    "        if (cp > 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126) :\n",
    "            return True\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat.startswith('P'):\n",
    "            return True\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7624cc8d-524f-45bc-8983-9559e2249939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', ',', 'I', 'submitted', 'my', 'resume', '.']\n"
     ]
    }
   ],
   "source": [
    "del_ascii = DEL_ASCII()\n",
    "print(del_ascii.do('   Today , I    submitted my rèsumé.   '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3c562-2c8b-4814-a2e6-2a27873be6ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 分词\n",
    "英文分词没有什么难度, 但是中文比较困难.常用中文分词方法:    \n",
    "1. 基于字符串匹配的分词方法\n",
    "又称为机械分词方法, 首先需要定义一个词表,表中包含当前语料中的全部词语. 然后按照一定的规则扫描待分词的文本, 匹配到表中的词语就把它切分开来.扫描规则可分为3种: 正向最大匹配, 即从开头向结尾扫描; 逆向最大扫描, 即从结尾向开头扫描;最少切分,即尝试每句话切分出最少的词语.  \n",
    "2. 基于统计的分词方法.\n",
    "在一大段语料中统计字与字或者词与词的上下文关系,统计字或词共同出现的次数.然后对于要切分的文本,可以按照这个已经统计到的出现次数,选择概率尽可能大的切分方法.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a500502-4718-4fb9-ad70-284ef5cb3e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSpliter(object):\n",
    "    def __init__(self, corpus_path, encoding='utf-8', max_load_word_length=4):\n",
    "        self.dict = {}\n",
    "        self.dict2 = {}\n",
    "        self.max_word_length = 1\n",
    "        begin_time = time.time()\n",
    "        \n",
    "        with open(corpus_path, 'r',encoding=encoding) as f:\n",
    "            for l in f:\n",
    "                l.replace('[','')\n",
    "                l.replace(']','')\n",
    "                wds = l.strip().split(' ')\n",
    "                last_wd = ''\n",
    "                for i in range(1, len(wds)): \n",
    "                    try:\n",
    "                        wd, wtype = wds[i].split('/')\n",
    "                    except:\n",
    "                        continue\n",
    "                    if len(wd) == 0 or len(wd) > max_load_word_length or not wd.isalpha():\n",
    "                        continue\n",
    "                    if wd not in self.dict:\n",
    "                        self.dict[wd] = 0\n",
    "                        if len(wd) > self.max_word_length:\n",
    "                            self.max_word_length = len(wd)\n",
    "                            print(f'max_load_word_length={self.max_word_length} ,word is {wd}')\n",
    "                    self.dict[wd] += 1\n",
    "                    if last_wd:\n",
    "                        if last_wd+':'+wd not in self.dict2:\n",
    "                            self.dict2[last_wd+':'+wd] = 0\n",
    "                        self.dict2[last_wd+':'+wd] += 1\n",
    "                    last_wd = wd\n",
    "                self.words_cnt = 0\n",
    "                max_c = 0\n",
    "                for wd in self.dict:\n",
    "                    self.words_cnt += self.dict[wd]\n",
    "                    if self.dict[wd] > max_c:\n",
    "                        max_c = self.dict[wd]\n",
    "                self.words2_cnt = sum(self.dict2.values())\n",
    "                print('load corpus finished!') \n",
    "                print(f'{len(self.dict)} words in dict and frequency is : {self.words_cnt} ')\n",
    "                print(f'{len(self.dict2)} words in dict2 and frequency is : {self.words2_cnt} ')\n",
    "                print(f'spend { time.time() - begin_time } seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f6a4b-7413-4bd4-af36-584921e7f7fd",
   "metadata": {},
   "source": [
    "### 词频-逆文本频率\n",
    "scikit-learn 中提供了计算TF-IDF的类 TfidfVectorizer.\n",
    "###  One-Hot 编码\n",
    "使用神经网络模型时,一般需要使用向量表示自然语言中的符号,也就是词或者字,最简单的表示方法是One-Hot编码. One-Hot编码是先遍历语料,找出所有的字或词,例如与10个词,对其进行编号,从1到10,每一个数字代表一个词语,转换成向量则每个词都是10维向量, 每个向量只有1位为1, 其余位为0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5403a8d-03e5-4c82-a883-51fa50a2c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://www.bilibili.com/video/BV1JV4y1A7NZ\")\n",
    "with open('raw-data/BV1JV4y1A7NZ.html','wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18681757-aee3-4c77-b448-3579d5905be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391516464"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def bv2av(bv: str) -> int:\n",
    "    data = requests.get(f'https://api.bilibili.com/x/web-interface/view?bvid={bv}').json()\n",
    "    return data['data']['aid']\n",
    "\n",
    "bv2av('BV19d4y1Y76N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa73bf9d-30bf-4efd-88d7-a24273dc8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_str = 'buvid3=E98157C4-E8C1-49A7-9B32-C63BCD1E06DB34761infoc; LIVE_BUVID=AUTO6016214362598664; i-wanna-go-back=-1; CURRENT_BLACKGAP=0; blackside_state=0; nostalgia_conf=-1; _uuid=EFD1B439-993B-511C-64109-5E9A36F10C69F89850infoc; buvid_fp_plain=undefined; DedeUserID=95074356; DedeUserID__ckMd5=37cd150f929f226f; b_ut=5; b_nut=100; bsource=search_baidu; rpdid=|(um~k))muk)0J\\'uYY))u)ml); buvid4=3A251516-8BE5-6A5D-63C1-660E9BBBC0E840460-022012611-cHGytz2Yu5EW%2FocyFzJD5Q%3D%3D; fingerprint=a6cdee9e65c10fbbbe73805f94c59260; hit-new-style-dyn=0; hit-dyn-v2=1; CURRENT_QUALITY=0; CURRENT_FNVAL=4048; is-2022-channel=1; buvid_fp=a6cdee9e65c10fbbbe73805f94c59260; SESSDATA=d09c509c%2C1687872939%2C3a062%2Ac2; bili_jct=a678f1cea82436c614dec4c039437139; sid=8hld0i7i; PVID=2; bp_video_offset_95074356=745096750678671500; b_lsid=10A86DEE5_18560A525F6; innersign=1'\n",
    "def get_replis(bv: str, next: int = 0, cookie: str = cookie_str):\n",
    "    oid = bv2av(bv)\n",
    "    # https://github.com/SocialSisterYi/bilibili-API-collect/blob/master/comment/list.md#%E8%8E%B7%E5%8F%96%E8%AF%84%E8%AE%BA%E5%8C%BA%E6%98%8E%E7%BB%86_%E7%BF%BB%E9%A1%B5%E5%8A%A0%E8%BD%BD\n",
    "    url = f\"https://api.bilibili.com/x/v2/reply/main?mode=3&next={next}&oid={oid}&type=1\"\n",
    "\n",
    "    payload={}\n",
    "    headers = {\n",
    "      'authority': 'api.bilibili.com',\n",
    "      'accept': 'application/json, text/plain, */*',\n",
    "      'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "      'cache-control': 'no-cache',\n",
    "      'cookie': cookie,\n",
    "      'origin': 'https://www.bilibili.com',\n",
    "      'pragma': 'no-cache',\n",
    "      'referer': f'https://www.bilibili.com/video/{bv}',\n",
    "      'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\", \"Google Chrome\";v=\"108\"',\n",
    "      'sec-ch-ua-mobile': '?0',\n",
    "      'sec-ch-ua-platform': '\"macOS\"',\n",
    "      'sec-fetch-dest': 'empty',\n",
    "      'sec-fetch-mode': 'cors',\n",
    "      'sec-fetch-site': 'same-site',\n",
    "      'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fd98ef1-cf38-4133-97b9-94b0248ee856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "bv_id = 'BV1JV4y1A7NZ'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "datas = get_replis(bv_id)\n",
    "data = datas[\"data\"]\n",
    "cursor = data[\"cursor\"]\n",
    "\n",
    "for r in data[\"replies\"]:\n",
    "    rpid = r['rpid'] \n",
    "    member = r['member']\n",
    "\n",
    "\n",
    "# while cursor['is_end'] == False:\n",
    "#     next = cursor['next']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d52c1fb2-f426-47ec-9480-83957e2cc77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cursor', 'replies', 'top', 'top_replies', 'up_selection', 'effects', 'assist', 'blacklist', 'vote', 'config', 'upper', 'control', 'note', 'callbacks'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[\"data\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e7d9a5d-da88-46b3-a966-3c42a6ebae78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['大家好我是盗月社的高饱饱，爱好是给沐上和树梢挖坑……啊不，准确地说是帮他俩减肥。\\n\\n《超级外卖员》第二季，在经历了两个多月的筹备和拍摄，以及二十多个选题因为突发状况被推翻重来、拍摄和上线反复延期之后，今天终于可以向大家说一声：我们回来啦！\\n2022的下半年对许多人来说都充满着动荡和不确定性。我们也体会到了什么是“计划永远赶不上变化”，常常是已经做足了所有的前期准备，就在要出发的前一天又接到目的地不能前往的消息。变化成为常态，于是就只能调整心态尽力应对，也因此不可避免地留下了许多遗憾。\\n\\n第一季我们用食物串联了一个个家庭，“饭盒虽小，却盛满了对彼此的思念。”这一季在策划的时候我们就在思考，我们还能带给大家些什么呢？在贵州为这一单奔波的几天几夜里，我们从足球队教练和小朋友们的身上，好像看到了一点答案。\\n《超级外卖员》的slogan是“超级外卖，传递食物和爱。”那么这一次，我们想再加上一个词——“超级外卖，传递食物、爱和希望。”有食物和爱的地方，就一定会有希望在吧！希望能用我们的镜头，去和大家一起寻找。\\n\\n盗月社还是一个小团队，但无论是每集都要“废掉一个”的沐上、树梢，还是每一个在幕后身兼数职、默默努力的小伙伴，都有一个共同的目标，那就是——一起做出更棒的内容！\\n回顾这段时间，在遵守政策、保证安全的前提下，我们盗月社的小伙伴们，一直在一起为实现一些事情努力，没有停止和放弃过。\\n也许还有很多不足和遗憾，但可以确认的是，我们的小团队在成长，这就是很棒的事情啦。也非常需要大家对我们的不足提出意见和建议，让我们在彼此陪伴的路上也一起进步吧～\\n\\n在这里也想再次感谢一下支持我们第2季的小伙伴@广汽丰田 威兰达。这回，终于不用像上一季那样“散尽家财”啦！其实一个需要付出很多时间精力、人力物力的节目，做一期不难，但只凭热爱能够持续做下去真的不容易。很幸运有这样一个品牌认可我们节目想要表达的东西、愿意陪我们去做这样的尝试。也让我们可以不断地去探索拓宽内容的边界。\\n\\n希望，可以在这个冬天给大家多一点的陪伴。就像今年夏天时那样！']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies = datas[\"data\"][\"top_replies\"]\n",
    "[ r[\"content\"]['message'] for r in replies ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a906a4-0b6c-4520-8e9c-20a0b4d7af11",
   "metadata": {},
   "source": [
    "curl 'https://api.bilibili.com/x/v2/reply/main?csrf=a678f1cea82436c614dec4c039437139&mode=3&next=0&oid=391516464&plat=1&seek_rpid=&type=1' \\\n",
    "  -H 'authority: api.bilibili.com' \\\n",
    "  -H 'accept: application/json, text/plain, */*' \\\n",
    "  -H 'accept-language: zh-CN,zh;q=0.9,en;q=0.8' \\\n",
    "  -H 'cache-control: no-cache' \\\n",
    "  -H $'cookie: buvid3=E98157C4-E8C1-49A7-9B32-C63BCD1E06DB34761infoc; LIVE_BUVID=AUTO6016214362598664; i-wanna-go-back=-1; CURRENT_BLACKGAP=0; blackside_state=0; nostalgia_conf=-1; _uuid=EFD1B439-993B-511C-64109-5E9A36F10C69F89850infoc; buvid_fp_plain=undefined; DedeUserID=95074356; DedeUserID__ckMd5=37cd150f929f226f; b_ut=5; b_nut=100; bsource=search_baidu; rpdid=|(um~k))muk)0J\\'uYY))u)ml); buvid4=3A251516-8BE5-6A5D-63C1-660E9BBBC0E840460-022012611-cHGytz2Yu5EW%2FocyFzJD5Q%3D%3D; fingerprint=a6cdee9e65c10fbbbe73805f94c59260; hit-new-style-dyn=0; hit-dyn-v2=1; CURRENT_QUALITY=0; CURRENT_FNVAL=4048; is-2022-channel=1; buvid_fp=a6cdee9e65c10fbbbe73805f94c59260; SESSDATA=d09c509c%2C1687872939%2C3a062%2Ac2; bili_jct=a678f1cea82436c614dec4c039437139; sid=8hld0i7i; PVID=2; bp_video_offset_95074356=745096750678671500; b_lsid=10A86DEE5_18560A525F6; innersign=1' \\\n",
    "  -H 'origin: https://www.bilibili.com' \\\n",
    "  -H 'pragma: no-cache' \\\n",
    "  -H 'referer: https://www.bilibili.com/video/BV19d4y1Y76N/?spm_id_from=333.1007.tianma.1-1-1.click&vd_source=2b27b91f674012c72f3dcabca6830272' \\\n",
    "  -H 'sec-ch-ua: \"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\", \"Google Chrome\";v=\"108\"' \\\n",
    "  -H 'sec-ch-ua-mobile: ?0' \\\n",
    "  -H 'sec-ch-ua-platform: \"macOS\"' \\\n",
    "  -H 'sec-fetch-dest: empty' \\\n",
    "  -H 'sec-fetch-mode: cors' \\\n",
    "  -H 'sec-fetch-site: same-site' \\\n",
    "  -H 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36' \\\n",
    "  --compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32df16a-5b50-4bf1-b9f8-4faa6ce5899e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
